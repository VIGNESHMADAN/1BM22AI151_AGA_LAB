{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*LAB-4 17-04-2025 DBN AND DBM*"
      ],
      "metadata": {
        "id": "zD4mZJFR6yJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DEEP BOLTZMAN MACHINE\n",
        "import numpy as np\n",
        "\n",
        "class DBM:\n",
        "    def __init__(self, num_visible, num_hidden_layers):\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_sizes = [num_visible // 2] * num_hidden_layers  # Simplified size for hidden layers\n",
        "\n",
        "        # Initialize weights for each layer\n",
        "        self.weights = [np.random.normal(0, 0.1, (self.hidden_sizes[i-1] if i > 0 else num_visible, self.hidden_sizes[i]))\n",
        "                        for i in range(num_hidden_layers)]\n",
        "        self.visible_bias = np.zeros(num_visible)\n",
        "        self.hidden_biases = [np.zeros(self.hidden_sizes[i]) for i in range(num_hidden_layers)]\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, data, layer_idx, direction):\n",
        "        if direction == 'hidden':\n",
        "            activations = np.dot(data, self.weights[layer_idx]) + self.hidden_biases[layer_idx]\n",
        "        elif direction == 'visible':\n",
        "            activations = np.dot(data, self.weights[layer_idx].T) + self.visible_bias\n",
        "        probabilities = self.sigmoid(activations)\n",
        "        samples = np.random.binomial(1, probabilities)\n",
        "        return probabilities, samples\n",
        "\n",
        "    def gibbs_sampling(self, visible_data, steps=10):\n",
        "        data = visible_data\n",
        "        for _ in range(steps):\n",
        "            # Forward pass through hidden layers\n",
        "            for layer_idx in range(self.num_hidden_layers):\n",
        "                hidden_prob, hidden_sample = self.sample(data, layer_idx, 'hidden')\n",
        "                data = hidden_prob\n",
        "\n",
        "            # Backward pass through visible layer\n",
        "            visible_prob, visible_sample = self.sample(data, 0, 'visible')\n",
        "            data = visible_prob\n",
        "\n",
        "        return visible_prob, data\n",
        "\n",
        "\n",
        "dbm = DBM(num_visible=4, num_hidden_layers=2)\n",
        "\n",
        "# Input visible data\n",
        "visible_data = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "\n",
        "# Gibbs sampling on the DBM\n",
        "visible_prob, final_visible_data = dbm.gibbs_sampling(visible_data, steps=10)\n",
        "\n",
        "print(\"Visible probabilities after Gibbs sampling:\")\n",
        "print(visible_prob)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtCcyCe57UOT",
        "outputId": "3173819c-57af-442d-9077-d35c6b3636e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visible probabilities after Gibbs sampling:\n",
            "[[0.49646967 0.48725011 0.48794753 0.50234454]\n",
            " [0.49646967 0.48725011 0.48794753 0.50234454]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        self.weights = np.random.normal(0, 0.1, (num_visible, num_hidden))\n",
        "        self.visible_bias = np.zeros(num_visible)\n",
        "        self.hidden_bias = np.zeros(num_hidden)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def train(self, data, num_epochs, learning_rate):\n",
        "        for epoch in range(num_epochs):\n",
        "            # Perform Gibbs sampling\n",
        "            hidden_probs, hidden_samples = self.sample_hidden(data)\n",
        "            visible_probs, visible_samples = self.sample_visible(hidden_samples)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.update_weights(data, visible_samples, hidden_probs, learning_rate)\n",
        "\n",
        "            # Output the progress\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}/{num_epochs} - Weight update complete\")\n",
        "\n",
        "    def sample_hidden(self, visible_data):\n",
        "        activations = np.dot(visible_data, self.weights) + self.hidden_bias\n",
        "        hidden_probs = self.sigmoid(activations)\n",
        "        return hidden_probs, np.random.binomial(1, hidden_probs)\n",
        "\n",
        "    def sample_visible(self, hidden_data):\n",
        "        activations = np.dot(hidden_data, self.weights.T) + self.visible_bias\n",
        "        visible_probs = self.sigmoid(activations)\n",
        "        return visible_probs, np.random.binomial(1, visible_probs)\n",
        "\n",
        "    def update_weights(self, data, visible_samples, hidden_probs, learning_rate):\n",
        "        positive_grad = np.dot(data.T, hidden_probs)\n",
        "        negative_grad = np.dot(visible_samples.T, hidden_probs)\n",
        "        self.weights += learning_rate * (positive_grad - negative_grad) / len(data)\n",
        "        self.visible_bias += learning_rate * np.mean(data - visible_samples, axis=0)\n",
        "        self.hidden_bias += learning_rate * np.mean(hidden_probs - hidden_probs, axis=0)\n",
        "\n",
        "\n",
        "class DBN:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.rbms = [RBM(layer_sizes[i], layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)]\n",
        "\n",
        "    def pretrain(self, data, num_epochs=10, learning_rate=0.1):\n",
        "        for i, rbm in enumerate(self.rbms):\n",
        "            print(f\"Pretraining layer {i + 1}...\")\n",
        "            rbm.train(data, num_epochs, learning_rate)\n",
        "\n",
        "            # Get hidden representations for the next layer\n",
        "            hidden_probs, _ = rbm.sample_hidden(data)\n",
        "            data = hidden_probs  # Pass the hidden layer activations to the next layer\n",
        "\n",
        "    def fine_tune(self, data, labels, learning_rate=0.1, num_epochs=10):\n",
        "        # Fine-tuning can be done with backpropagation (this part is simplified)\n",
        "        print(\"Fine-tuning process (placeholder)\")\n",
        "\n",
        "\n",
        "dbn = DBN([4, 3, 2])  # 4 visible units, 1st hidden layer with 3 units, 2nd hidden layer with 2 units\n",
        "data = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 0, 1, 1]])  # Dummy data\n",
        "\n",
        "# Pretrain DBN\n",
        "print(\"Starting pretraining...\")\n",
        "dbn.pretrain(data, num_epochs=50)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhegEuqk8iJf",
        "outputId": "062b6f8a-b897-4f51-f188-9969ca710b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pretraining...\n",
            "Pretraining layer 1...\n",
            "Epoch 0/50 - Weight update complete\n",
            "Epoch 10/50 - Weight update complete\n",
            "Epoch 20/50 - Weight update complete\n",
            "Epoch 30/50 - Weight update complete\n",
            "Epoch 40/50 - Weight update complete\n",
            "Pretraining layer 2...\n",
            "Epoch 0/50 - Weight update complete\n",
            "Epoch 10/50 - Weight update complete\n",
            "Epoch 20/50 - Weight update complete\n",
            "Epoch 30/50 - Weight update complete\n",
            "Epoch 40/50 - Weight update complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_units, hidden_units):\n",
        "        super(RBM, self).__init__()\n",
        "        self.visible_units = visible_units\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(visible_units, hidden_units) * 0.1)\n",
        "        self.bv = nn.Parameter(torch.zeros(visible_units))\n",
        "        self.bh = nn.Parameter(torch.zeros(hidden_units))\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        h_prob = torch.sigmoid(torch.matmul(v, self.W) + self.bh)\n",
        "        return h_prob\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        v_prob = torch.sigmoid(torch.matmul(h, self.W.t()) + self.bv)\n",
        "        return v_prob\n",
        "\n",
        "    def contrastive_divergence(self, v0, k=1):\n",
        "        v = v0\n",
        "        for _ in range(k):\n",
        "            h0 = self.sample_h(v)  # Initial hidden layer activations\n",
        "            v = self.sample_v(h0)  # Reconstructed visible layer\n",
        "        h1 = self.sample_h(v)  # Hidden layer after reconstruction\n",
        "\n",
        "\n",
        "        positive_grad = torch.matmul(v0.t(), h0)\n",
        "        negative_grad = torch.matmul(v.t(), h1)\n",
        "\n",
        "        dW = positive_grad - negative_grad\n",
        "        dbv = torch.sum(v0 - v, 0)\n",
        "        dbh = torch.sum(h0 - h1, 0)\n",
        "\n",
        "        return dW, dbv, dbh\n",
        "\n",
        "    def train_rbm(self, data, learning_rate=0.01, batch_size=10, epochs=10, k=1):\n",
        "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch_data = data[i:i+batch_size]\n",
        "                v0 = Variable(torch.FloatTensor(batch_data))\n",
        "\n",
        "                dW, dbv, dbh = self.contrastive_divergence(v0, k)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                self.W.grad = dW\n",
        "                self.bv.grad = dbv\n",
        "                self.bh.grad = dbh\n",
        "                optimizer.step()\n",
        "            print(f\"Epoch {epoch+1}/{epochs} completed\")\n",
        "\n",
        "    def forward(self, v):\n",
        "        return self.sample_h(v)\n",
        "\n",
        "\n",
        "class DBN(nn.Module):\n",
        "    def __init__(self, layer_sizes, n_classes):\n",
        "        super(DBN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(RBM(layer_sizes[i], layer_sizes[i+1]))\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(layer_sizes[-1], n_classes)\n",
        "\n",
        "    def pretrain(self, data, learning_rate=0.01, batch_size=10, epochs=10, k=1):\n",
        "        for layer in self.layers:\n",
        "            layer.train_rbm(data, learning_rate, batch_size, epochs, k)\n",
        "\n",
        "            data = layer.sample_h(torch.FloatTensor(data)).detach().numpy()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.sample_h(x)\n",
        "\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "\n",
        "def prepare_data(loader):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for images, targets in loader:\n",
        "        data.append(images.view(images.size(0), -1).numpy())\n",
        "        labels.append(targets.numpy())\n",
        "    return np.concatenate(data), np.concatenate(labels)\n",
        "\n",
        "train_data, train_labels = prepare_data(train_loader)\n",
        "test_data, test_labels = prepare_data(test_loader)\n",
        "\n",
        "\n",
        "dbn = DBN([784, 512, 256, 128], 10)  # 784 -> 512 -> 256 -> 128 -> 10 (for MNIST)\n",
        "\n",
        "\n",
        "dbn.pretrain(train_data, learning_rate=0.01, batch_size=10, epochs=5, k=1)\n",
        "\n",
        "# Fine-tune the DBN using supervised learning\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(dbn.parameters(), lr=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    dbn.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.view(images.size(0), -1)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "\n",
        "        outputs = dbn(images)\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "\n",
        "dbn.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.view(images.size(0), -1)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "\n",
        "        outputs = dbn(images)\n",
        "\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7007OhBnAYp3",
        "outputId": "b19c87b4-3e93-4dd0-9e18-714934194e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 51.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.70MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.5MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.42MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 completed\n",
            "Epoch 2/5 completed\n",
            "Epoch 3/5 completed\n",
            "Epoch 4/5 completed\n",
            "Epoch 5/5 completed\n",
            "Epoch 1/5 completed\n",
            "Epoch 2/5 completed\n",
            "Epoch 3/5 completed\n",
            "Epoch 4/5 completed\n",
            "Epoch 5/5 completed\n",
            "Epoch 1/5 completed\n",
            "Epoch 2/5 completed\n",
            "Epoch 3/5 completed\n",
            "Epoch 4/5 completed\n",
            "Epoch 5/5 completed\n",
            "Epoch [1/10], Loss: 2.3490, Accuracy: 10.27%\n",
            "Epoch [2/10], Loss: 2.3458, Accuracy: 10.34%\n",
            "Epoch [3/10], Loss: 2.3463, Accuracy: 10.27%\n",
            "Epoch [4/10], Loss: 2.3457, Accuracy: 10.17%\n",
            "Epoch [5/10], Loss: 2.3481, Accuracy: 10.02%\n",
            "Epoch [6/10], Loss: 2.3469, Accuracy: 10.14%\n",
            "Epoch [7/10], Loss: 2.3465, Accuracy: 10.20%\n",
            "Epoch [8/10], Loss: 2.3486, Accuracy: 10.18%\n",
            "Epoch [9/10], Loss: 2.3305, Accuracy: 10.48%\n",
            "Epoch [10/10], Loss: 2.3018, Accuracy: 11.22%\n",
            "Test Accuracy: 11.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
        "                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
        "                'hours_per_week', 'native_country', 'income']\n",
        "\n",
        "\n",
        "data = pd.read_csv(url, names=column_names, na_values=\" ?\", sep=',\\s+', engine='python')\n",
        "\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "\n",
        "categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "X = data.drop('income', axis=1).values\n",
        "y = data['income'].values\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "n_hidden_1 = 128\n",
        "n_hidden_2 = 64\n",
        "\n",
        "\n",
        "rbm1 = BernoulliRBM(n_components=n_hidden_1, learning_rate=0.1, n_iter=10, random_state=42)\n",
        "X_train_rbm1 = rbm1.fit_transform(X_train)\n",
        "\n",
        "rbm2 = BernoulliRBM(n_components=n_hidden_2, learning_rate=0.1, n_iter=10, random_state=42)\n",
        "X_train_rbm2 = rbm2.fit_transform(X_train_rbm1)\n",
        "\n",
        "\n",
        "model_dbn = Sequential()\n",
        "model_dbn.add(Dense(64, input_dim=n_hidden_2, activation='relu'))\n",
        "model_dbn.add(Dense(32, activation='relu'))\n",
        "model_dbn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_dbn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_dbn.fit(X_train_rbm2, y_train, epochs=10, batch_size=128)\n",
        "\n",
        "X_test_rbm1 = rbm1.transform(X_test)\n",
        "X_test_rbm2 = rbm2.transform(X_test_rbm1)\n",
        "\n",
        "test_loss, test_acc = model_dbn.evaluate(X_test_rbm2, y_test)\n",
        "print(f\"Test accuracy for DBN: {test_acc * 100:.2f}%\")\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=10, solver='adam', random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "traditional_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy for traditional DNN: {traditional_acc * 100:.2f}%\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(rbm1.components_[i].reshape(2, 7), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"RBM1 Learned Features (reshaped 2x7)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "wG0UNDZyAdn_",
        "outputId": "a4089be3-c25a-453d-c2c6-b56510c015bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7462 - loss: 0.5735\n",
            "Epoch 2/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7597 - loss: 0.4954\n",
            "Epoch 3/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.5024\n",
            "Epoch 4/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7553 - loss: 0.4968\n",
            "Epoch 5/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7549 - loss: 0.4957\n",
            "Epoch 6/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7563 - loss: 0.4943\n",
            "Epoch 7/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7559 - loss: 0.4977\n",
            "Epoch 8/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7588 - loss: 0.4935\n",
            "Epoch 9/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4905\n",
            "Epoch 10/10\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7559 - loss: 0.4940\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.4923\n",
            "Test accuracy for DBN: 76.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for traditional DNN: 84.97%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}